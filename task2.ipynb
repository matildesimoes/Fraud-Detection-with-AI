{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnologias Utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de analisar e tratar todos os dados, vamos passar para as etapas de modelação e avaliação. Assim, é necessário voltar a ler os dados do **dataset** tratado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = pd.read_csv('CreditCardTransactions/training_dataset.csv')\n",
    "training_classes = pd.read_csv('CreditCardTransactions/training_classes.csv')\n",
    "testing_inputs = pd.read_csv('CreditCardTransactions/testing_dataset.csv')\n",
    "testing_classes = pd.read_csv('CreditCardTransactions/testing_classes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelação\n",
    "\n",
    "Em primeiro lugar, desenvolvemos um modelo de classificação baseado numa **Árvore de Decisão**. Tendo em conta que a variável-alvo é categórica, pelo que este é um problema de classificação e o objetivo do modelo é classificar registos, tomando decisões a partir dos atributos de cada cliente, este algoritmo considera-se adequado para o efeito.\n",
    "\n",
    "Em segundo lugar, implementamos uma **Rede Neuronal**, que é um modelo de *deep learning* com capacidade para aprender a partir dos dados, sendo capaz de identificar padrões complexos e de fazer previsões precisas.\n",
    "\n",
    "Para além disto, construímos um modelo de classificação assente numa ***Support Vector Machine*** (SVM). Esta abordagem tenta encontrar o hiperplano que melhor separa os registos das diferentes classes, maximizando a margem entre esse hiperplano e os registos das duas classes.\n",
    "\n",
    "Por último, apresentamos um ***Random Forest Classifier***, que, sendo um modelo de *ensembling*, combina várias árvores de decisão para classificar melhor os dados.\n",
    "\n",
    "Os modelos devem ser treinados com um conjunto de dados de treino e testados com um conjunto de dados de teste. O conjunto de teste vai corresponder a 25% do *dataset*.\n",
    "\n",
    "Para a construção de alguns modelos, como a rede neuronal, o *k-nearest neighbors* e o *support vector machine*, é necessário normalizar os dados de maneira a que tenham média 0 e variância 1, aplicando este processo ao conjunto de treino e ao conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- unique_merchants_count\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      3\u001b[0m training_inputs_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(training_inputs)\n\u001b[1;32m----> 5\u001b[0m testing_inputs_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtesting_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    545\u001b[0m ):\n\u001b[0;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m     )\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- unique_merchants_count\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "training_inputs_scaled = scaler.fit_transform(training_inputs)\n",
    "\n",
    "testing_inputs_scaled = scaler.transform(testing_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "training_inputs, training_classes = smote.fit_resample(training_inputs, training_classes)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "training_inputs_scaled = scaler.fit_transform(training_inputs)\n",
    "\n",
    "testing_inputs_scaled = scaler.transform(testing_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "cross_validation = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Definir o modelo Decision Tree e o Pipeline\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', smote),                # Aplicar SMOTE\n",
    "    ('model', decision_tree)         # Modelo a ser treinado\n",
    "])\n",
    "\n",
    "# Definir os hiperparâmetros para o modelo dentro do pipeline\n",
    "decision_tree_parameter_grid = {\n",
    "    'model__max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],  # Prefixo \"model__\" por causa do pipeline\n",
    "    'model__max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV\n",
    "decision_tree_grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=decision_tree_parameter_grid,\n",
    "    cv=cross_validation,\n",
    "    n_jobs=-1,\n",
    "    scoring='auc'  # Alterar para outra métrica, se necessário\n",
    ")\n",
    "\n",
    "# Ajustar o GridSearchCV\n",
    "decision_tree_grid_search.fit(training_inputs, training_classes)\n",
    "\n",
    "# Exibir os resultados\n",
    "print('Decision Tree Best Score: {:.2f}%'.format(decision_tree_grid_search.best_score_ * 100))\n",
    "print('Decision Tree Best Parameters:', decision_tree_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_decision_tree_params = decision_tree_grid_search.best_params_\n",
    "best_decision_tree = DecisionTreeClassifier(max_depth = 10, max_features = 10, random_state = 42)\n",
    "best_decision_tree.fit(training_inputs, training_classes)\n",
    "best_decision_tree.score(testing_inputs, testing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = MLPClassifier(max_iter = 1000, random_state = 13)\n",
    "\n",
    "\n",
    "neural_network_parameter_grid = {\n",
    "    'hidden_layer_sizes': [(5, ), (6, ), (7, ), (5, 4), (5, 5), (6, 4), (6, 5), (6, 6)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "}\n",
    "\n",
    "neural_network_grid_search = GridSearchCV(neural_network, neural_network_parameter_grid, n_jobs = -1)\n",
    "\n",
    "neural_network_grid_search.fit(training_inputs_scaled, training_classes)\n",
    "\n",
    "print('Neural Network Best Score: {:.2f}%'.format(neural_network_grid_search.best_score_ * 100))\n",
    "print('Neural Network Best Parameters:', neural_network_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neural_network_params = neural_network_grid_search.best_params_\n",
    "best_neural_network = MLPClassifier(hidden_layer_sizes = best_neural_network_params['hidden_layer_sizes'], activation = best_neural_network_params['activation'], solver = best_neural_network_params['solver'], random_state = 13)\n",
    "best_neural_network.fit(training_inputs_scaled, training_classes)\n",
    "best_neural_network.score(testing_inputs_scaled, testing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 13)\n",
    "\n",
    "svm_parameter_grid = {\n",
    "    'C': [0.9, 1.0, 1.1],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm, svm_parameter_grid, cv = cross_validation, n_jobs = -1)\n",
    "\n",
    "svm_grid_search.fit(training_inputs_scaled, training_classes)\n",
    "\n",
    "print(\"Support Vector Machine Best Score: {:.2f}%\".format(svm_grid_search.best_score_ * 100))\n",
    "print(\"Support Vector Machine Best Parameters:\", svm_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_params = svm_grid_search.best_params_\n",
    "best_svm = SVC(C = best_svm_params['C'], kernel = best_svm_params['kernel'], gamma = best_svm_params['gamma'], random_state = 13)\n",
    "best_svm.fit(training_inputs_scaled, training_classes)\n",
    "best_svm.score(testing_inputs_scaled, testing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state = 13)\n",
    "\n",
    "random_forest_parameter_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [30, 40],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "random_forest_grid_search = GridSearchCV(random_forest, random_forest_parameter_grid, cv = cross_validation, n_jobs = -1)\n",
    "\n",
    "random_forest_grid_search.fit(training_inputs, training_classes)\n",
    "\n",
    "print('Random Forest Best Score: {:.2f}%'.format(random_forest_grid_search.best_score_ * 100))\n",
    "print('Random Forest Best Parameters:', random_forest_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest_params = random_forest_grid_search.best_params_\n",
    "best_random_forest = RandomForestClassifier(n_estimators = best_random_forest_params['n_estimators'], max_depth = best_random_forest_params['max_depth'], min_samples_split = best_random_forest_params['min_samples_split'], min_samples_leaf = best_random_forest_params['min_samples_leaf'], max_features = best_random_forest_params['max_features'])\n",
    "best_random_forest.fit(training_inputs, training_classes)\n",
    "best_random_forest.score(testing_inputs, testing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Inicializar o XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=13,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Definir o espaço de hiperparâmetros para XGBoost\n",
    "xgb_parameter_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "}\n",
    "\n",
    "# Configurar o GridSearchCV para XGBoost\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=xgb_parameter_grid,\n",
    "    cv=cross_validation,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Executar o Grid Search\n",
    "xgb_grid_search.fit(training_inputs_scaled, training_classes)\n",
    "\n",
    "# Exibir os melhores parâmetros e a melhor pontuação\n",
    "print('XGBoost Best Score: {:.2f}%'.format(xgb_grid_search.best_score_ * 100))\n",
    "print('XGBoost Best Parameters:', xgb_grid_search.best_params_)\n",
    "\n",
    "# Utilizar o melhor modelo para prever no conjunto de teste\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "y_pred = best_xgb_model.predict(testing_inputs_scaled)\n",
    "\n",
    "# Avaliar a acurácia no conjunto de teste\n",
    "test_accuracy = accuracy_score(testing_classes, y_pred)\n",
    "print('XGBoost Test Accuracy: {:.2f}%'.format(test_accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_decision_tree_pred = best_decision_tree.predict(testing_inputs)\n",
    "\n",
    "print(classification_report(testing_classes, best_decision_tree_pred))\n",
    "\n",
    "best_decision_tree_confusion_matrix = confusion_matrix(testing_classes, best_decision_tree_pred)\n",
    "ConfusionMatrixDisplay(best_decision_tree_confusion_matrix).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neural_network_pred = best_neural_network.predict(testing_inputs_scaled)\n",
    "\n",
    "print(classification_report(testing_classes, best_neural_network_pred))\n",
    "\n",
    "best_neural_network_confusion_matrix = confusion_matrix(testing_classes, best_neural_network_pred)\n",
    "ConfusionMatrixDisplay(best_neural_network_confusion_matrix).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_pred = best_svm.predict(testing_inputs_scaled)\n",
    "\n",
    "print(classification_report(testing_classes, best_svm_pred))\n",
    "\n",
    "best_svm_confusion_matrix = confusion_matrix(testing_classes, best_svm_pred)\n",
    "ConfusionMatrixDisplay(best_svm_confusion_matrix).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest_pred = best_random_forest.predict(testing_inputs)\n",
    "\n",
    "print(classification_report(testing_classes, best_random_forest_pred))\n",
    "\n",
    "best_random_forest_confusion_matrix = confusion_matrix(testing_classes, best_random_forest_pred)\n",
    "ConfusionMatrixDisplay(best_random_forest_confusion_matrix).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_confusion_matrix = confusion_matrix(testing_classes, y_pred)\n",
    "\n",
    "# Exibir a Matriz de Confusão\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=xgb_confusion_matrix, display_labels=best_xgb_model.classes_).plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
